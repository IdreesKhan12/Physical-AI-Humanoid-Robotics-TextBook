"use strict";(globalThis.webpackChunktemp_docusaurus_init=globalThis.webpackChunktemp_docusaurus_init||[]).push([[1364],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},8832:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/llm-cognitive-planning","title":"Cognitive Planning with LLMs","description":"Short Overview","source":"@site/docs/module-4/llm-cognitive-planning.mdx","sourceDirName":"module-4","slug":"/module-4/llm-cognitive-planning","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-4/llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/IdreesKhan12/Physical-AI-Humanoid-Robotics-TextBook/tree/main/docs/module-4/llm-cognitive-planning.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Cognitive Planning with LLMs","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Conversational Robotics Stack","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-4/conversational-ai"}}');var o=t(4848),s=t(8453);const a={title:"Cognitive Planning with LLMs",sidebar_position:2},r=void 0,l={},c=[{value:"Short Overview",id:"short-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Main Sections",id:"main-sections",level:2},{value:"1. The Brain of the Robot: LLMs for High-Level Control",id:"1-the-brain-of-the-robot-llms-for-high-level-control",level:3},{value:"LLM&#39;s Role in Translating Intent to Action",id:"llms-role-in-translating-intent-to-action",level:4},{value:"2. Prompt Engineering for Robot Action Generation",id:"2-prompt-engineering-for-robot-action-generation",level:3},{value:"Template for Instructing the LLM on Valid ROS 2 Actions (Robot&#39;s API)",id:"template-for-instructing-the-llm-on-valid-ros-2-actions-robots-api",level:4},{value:"4. LLM Communication with the ROS 2 System",id:"4-llm-communication-with-the-ros-2-system",level:3},{value:"Summary Key Points",id:"summary-key-points",level:2},{value:"Reading/Research References (APA 7th Edition)",id:"readingresearch-references-apa-7th-edition",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"short-overview",children:"Short Overview"}),"\n",(0,o.jsxs)(n.p,{children:['This chapter focuses on using Large Language Models (LLMs) to bridge human intention with technical robot actions, a critical component of high-level robot control and decision-making. It defines the LLM\'s role in translating abstract goals (like "Clean the room") into concrete sequences of low-level ROS 2 actions. The chapter provides guidance on ',(0,o.jsx)(n.strong,{children:"prompt engineering"})," with a template for instructing the LLM on valid robot APIs and demonstrates simplified structured ",(0,o.jsx)(n.strong,{children:"action sequences"})," (e.g., JSON or YAML) as LLM output."]}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:['Clearly define the role of the LLM: translating a goal ("Clean the room") into a sequence of low-level ',(0,o.jsx)(n.strong,{children:"ROS 2 actions"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"Provide a template for the system prompt used to instruct the LLM on valid ROS 2 actions (the robot's API)."}),"\n",(0,o.jsx)(n.li,{children:"Demonstrate a simplified example of the LLM outputting a structured action sequence (e.g., JSON or YAML)."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"main-sections",children:"Main Sections"}),"\n",(0,o.jsx)(n.h3,{id:"1-the-brain-of-the-robot-llms-for-high-level-control",children:"1. The Brain of the Robot: LLMs for High-Level Control"}),"\n",(0,o.jsxs)(n.p,{children:['While lower-level components handle perception and motor control, intelligent robots need a "brain" to understand high-level human commands and translate them into a series of executable actions. Large Language Models (LLMs) are uniquely suited for this ',(0,o.jsx)(n.strong,{children:"cognitive planning"})," role due to their ability to understand natural language, reason, and generate structured text. This chapter emphasizes the ",(0,o.jsx)(n.strong,{children:"reasoning"})," and ",(0,o.jsx)(n.strong,{children:"translation"})," step within the broader Vision-Language-Action (VLA) loop."]}),"\n",(0,o.jsx)(n.h4,{id:"llms-role-in-translating-intent-to-action",children:"LLM's Role in Translating Intent to Action"}),"\n",(0,o.jsx)(n.p,{children:'The core role of an LLM in cognitive planning is to convert an abstract, high-level human goal (e.g., "Take out the trash," "Make me a coffee") into a detailed, ordered sequence of technical, low-level robot actions. These low-level actions are typically defined by the robot\'s existing ROS 2 API (e.g., specific ROS 2 services or action goals).'}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Example"}),': A human says, "Clean the room." The LLM translates this into:',"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'NAVIGATE_TO_LOCATION {location: "kitchen"}'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'GRASP_OBJECT {object_type: "trash_bag"}'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'NAVIGATE_TO_LOCATION {location: "door_outside"}'})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:'DEPOSIT_OBJECT {object_type: "trash_bag"}'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-prompt-engineering-for-robot-action-generation",children:"2. Prompt Engineering for Robot Action Generation"}),"\n",(0,o.jsxs)(n.p,{children:["The effectiveness of an LLM in cognitive planning heavily relies on ",(0,o.jsx)(n.strong,{children:"prompt engineering"}),"\u2014crafting the input (the \"system prompt\") to guide the LLM's behavior and ensure its output adheres to the robot's technical capabilities. The system prompt defines the LLM's role, constraints, and the format for its output."]}),"\n",(0,o.jsx)(n.h4,{id:"template-for-instructing-the-llm-on-valid-ros-2-actions-robots-api",children:"Template for Instructing the LLM on Valid ROS 2 Actions (Robot's API)"}),"\n",(0,o.jsx)(n.p,{children:"A robust system prompt should clearly define the robot's available actions, their parameters, and the expected output format."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-markdown",children:'You are a robotic task planner. Your goal is to translate high-level human commands into a sequence of low-level ROS 2 actions.\n\nAvailable ROS 2 Actions (Robot API):\n- NAVIGATE_TO_LOCATION(location: string): Navigates the robot to a predefined location.\n  - Valid locations: "kitchen", "living_room", "bedroom", "door_outside"\n- GRASP_OBJECT(object_type: string): Grasps an object of a specified type.\n  - Valid object_types: "cup", "book", "trash_bag", "remote"\n- DEPOSIT_OBJECT(object_type: string, location: string): Deposits a held object at a specified location.\n  - Valid object_types: "cup", "book", "trash_bag", "remote" (must be currently held)\n  - Valid locations: "table", "shelf", "bin", "floor"\n- TAKE_PHOTO(camera_name: string, save_path: string): Takes a photo with a specified camera and saves it.\n  - Valid camera_names: "front_camera", "left_camera", "right_camera"\n\nOutput Format:\nRespond ONLY with a JSON array of action objects. Each object must have an "action" field (corresponding to an Available ROS 2 Action) and an "params" field (a dictionary of parameters for that action). Do NOT include any other text or explanation.\n\nExample Input: "Go to the kitchen and grab the cup, then put it on the table."\n\nExample Output:\n```json\n[\n  {\n    "action": "NAVIGATE_TO_LOCATION",\n    "params": {\n      "location": "kitchen"\n    }\n  },\n  {\n    "action": "GRASP_OBJECT",\n    "params": {\n      "object_type": "cup"\n    }\n  },\n  {\n    "action": "DEPOSIT_OBJECT",\n    "params": {\n      "object_type": "cup",\n      "location": "table"\n    }\n  }\n]\n'})}),"\n",(0,o.jsx)(n.p,{children:"Human Command: [User's high-level command goes here]"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'\n### 3. Demonstrating Structured Action Sequence Output\n\nThe LLM\'s output needs to be parsable and executable by the robot\'s control system. A structured format like JSON or YAML is ideal for this.\n\n#### Simplified Example: LLM Output to ROS 2 System\n\nBuilding on the previous example:\n\n**Human Command**: "Clean the room by finding the trash and putting it outside."\n\n**LLM System Prompt**: (The template provided above)\n\n**LLM Output (JSON)**:\n```json\n[\n  {\n    "action": "NAVIGATE_TO_LOCATION",\n    "params": {\n      "location": "living_room"\n    }\n  },\n  {\n    "action": "GRASP_OBJECT",\n    "params": {\n      "object_type": "trash_bag"\n    }\n  },\n  {\n    "action": "NAVIGATE_TO_LOCATION",\n    "params": {\n      "location": "door_outside"\n    }\n  },\n  {\n    "action": "DEPOSIT_OBJECT",\n    "params": {\n      "object_type": "trash_bag",\n      "location": "bin"\n    }\n  }\n]\n'})}),"\n",(0,o.jsx)(n.p,{children:"This structured output can then be parsed by a Python or C++ component in the ROS 2 system. Each action in the sequence is then dispatched to the corresponding ROS 2 service or action client for execution."}),"\n",(0,o.jsx)(n.h3,{id:"4-llm-communication-with-the-ros-2-system",children:"4. LLM Communication with the ROS 2 System"}),"\n",(0,o.jsx)(n.p,{children:"The connection between the LLM (which performs cognitive planning) and the ROS 2 system (which executes actions) is a crucial interface. This communication typically occurs through ROS 2 service calls or action goals."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mechanism"}),": Once the LLM generates a structured action sequence (e.g., the JSON array above), a dedicated ROS 2 node is responsible for:","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parsing"}),": Interpreting the LLM's output."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dispatching"}),": Calling the appropriate ROS 2 services (for instantaneous actions) or sending action goals (for long-running, feedback-enabled actions) as defined in the robot's API."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback"}),": Monitoring the execution of these ROS 2 actions and potentially feeding status updates back to the LLM for replanning if an action fails."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This discussion emphasizes the ",(0,o.jsx)(n.strong,{children:"reasoning"})," and ",(0,o.jsx)(n.strong,{children:"translation"})," step of the VLA loop, where the LLM acts as the high-level planner, orchestrating a series of low-level robot behaviors."]}),"\n",(0,o.jsx)(n.h2,{id:"summary-key-points",children:"Summary Key Points"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["LLMs are critical for ",(0,o.jsx)(n.strong,{children:"cognitive planning"})," in robotics, translating high-level human goals into low-level ROS 2 actions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prompt engineering"})," is essential to instruct the LLM on the robot's available API and ensure structured output."]}),"\n",(0,o.jsxs)(n.li,{children:["LLMs generate structured ",(0,o.jsx)(n.strong,{children:"action sequences"})," (e.g., JSON, YAML) that are parsed and executed by the ROS 2 system."]}),"\n",(0,o.jsxs)(n.li,{children:["Communication between the LLM and ROS 2 is typically handled via ",(0,o.jsx)(n.strong,{children:"ROS 2 Service calls"})," or ",(0,o.jsx)(n.strong,{children:"Action Goals"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["This process is a key reasoning and translation step within the ",(0,o.jsx)(n.strong,{children:"VLA loop"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"readingresearch-references-apa-7th-edition",children:"Reading/Research References (APA 7th Edition)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Huang, W., et al. (2022). ",(0,o.jsx)(n.em,{children:"Inner Monologue: Empowering LLMs as Active Reasoners for (Embodied) Agentic Tasks"}),". arXiv preprint arXiv:2207.05612."]}),"\n",(0,o.jsxs)(n.li,{children:["Google Research. (2023). ",(0,o.jsx)(n.em,{children:"SayCan: Learning Language Models for Robotic Manipulation"}),". Retrieved from ",(0,o.jsx)(n.a,{href:"https://ai.googleblog.com/2022/03/saycan-learning-language-models-for.html",children:"https://ai.googleblog.com/2022/03/saycan-learning-language-models-for.html"})]}),"\n",(0,o.jsxs)(n.li,{children:["ROS 2 Documentation. (n.d.). ",(0,o.jsx)(n.em,{children:"ROS 2 Actions"}),". Retrieved from ",(0,o.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Actions/Understanding-ROS2-Actions.html",children:"https://docs.ros.org/en/humble/Tutorials/Actions/Understanding-ROS2-Actions.html"})]}),"\n",(0,o.jsxs)(n.li,{children:["Wen, Y., et al. (2022). ",(0,o.jsx)(n.em,{children:"Robotics with Large Language Models: A Review"}),". arXiv preprint arXiv:2209.05275."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}}}]);