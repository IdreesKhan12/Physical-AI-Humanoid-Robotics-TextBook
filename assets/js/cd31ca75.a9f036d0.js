"use strict";(globalThis.webpackChunktemp_docusaurus_init=globalThis.webpackChunktemp_docusaurus_init||[]).push([[8210],{7166:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-3/nav2-sim-to-real","title":"Training and Sim-to-Real","description":"Short Overview","source":"@site/docs/module-3/nav2-sim-to-real.mdx","sourceDirName":"module-3","slug":"/module-3/nav2-sim-to-real","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-3/nav2-sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/IdreesKhan12/Physical-AI-Humanoid-Robotics-TextBook/tree/main/docs/module-3/nav2-sim-to-real.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Training and Sim-to-Real","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Perception and Navigation","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-3/isaac-ros-perception"},"next":{"title":"Conversational Robotics Stack","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-4/conversational-ai"}}');var t=i(4848),r=i(8453);const a={title:"Training and Sim-to-Real",sidebar_position:3},s=void 0,l={},d=[{value:"Short Overview",id:"short-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Main Sections",id:"main-sections",level:2},{value:"1. Reinforcement Learning (RL) for Robot Control",id:"1-reinforcement-learning-rl-for-robot-control",level:3},{value:"2. The Sim-to-Real Gap and Domain Randomization",id:"2-the-sim-to-real-gap-and-domain-randomization",level:3},{value:"What is the Sim-to-Real Gap?",id:"what-is-the-sim-to-real-gap",level:4},{value:"Mitigating the Sim-to-Real Gap with Domain Randomization",id:"mitigating-the-sim-to-real-gap-with-domain-randomization",level:4},{value:"3. Deployment Workflow: From Training to Jetson Orin",id:"3-deployment-workflow-from-training-to-jetson-orin",level:3},{value:"Workflow Stages:",id:"workflow-stages",level:4},{value:"4. The Critical Role of Latency",id:"4-the-critical-role-of-latency",level:3},{value:"Summary Key Points",id:"summary-key-points",level:2},{value:"Reading/Research References (APA 7th Edition)",id:"readingresearch-references-apa-7th-edition",level:2}];function c(e){const n={a:"a",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"short-overview",children:"Short Overview"}),"\n",(0,t.jsxs)(n.p,{children:["This chapter explores the crucial process of training robot controllers in simulation and effectively transferring them to real robots. It introduces ",(0,t.jsx)(n.strong,{children:"Reinforcement Learning (RL)"})," as a primary control method, defines the ",(0,t.jsx)(n.strong,{children:"Sim-to-Real Gap"})," and mitigation techniques like ",(0,t.jsx)(n.strong,{children:"domain randomization"}),", and outlines the workflow for deploying trained models onto edge hardware like the ",(0,t.jsx)(n.strong,{children:"Jetson Orin"}),". The chapter also highlights the importance of ",(0,t.jsx)(n.strong,{children:"latency"})," in real-time robot control."]}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Briefly explain ",(0,t.jsx)(n.strong,{children:"Reinforcement Learning (RL)"})," as a primary control method in robotics."]}),"\n",(0,t.jsxs)(n.li,{children:["Clearly define the ",(0,t.jsx)(n.strong,{children:"Sim-to-Real Gap"})," (domain randomization) and techniques to mitigate it."]}),"\n",(0,t.jsxs)(n.li,{children:["Outline the steps to train in the cloud/workstation, download the weights, and flash them onto the local ",(0,t.jsx)(n.strong,{children:"Jetson Orin"})," kit."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"main-sections",children:"Main Sections"}),"\n",(0,t.jsx)(n.h3,{id:"1-reinforcement-learning-rl-for-robot-control",children:"1. Reinforcement Learning (RL) for Robot Control"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reinforcement Learning (RL)"})," is a powerful machine learning paradigm where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward. In robotics, RL is increasingly used to train complex robot behaviors and controllers, especially for tasks that are difficult to program manually, such as grasping, locomotion, and navigation in dynamic environments."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"How RL works"}),": An RL agent interacts with its environment, taking actions and observing the resulting states and rewards. Through a process of trial and error, often guided by deep neural networks (Deep Reinforcement Learning), the agent learns an optimal policy\u2014a mapping from states to actions\u2014that maximizes its long-term reward."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-the-sim-to-real-gap-and-domain-randomization",children:"2. The Sim-to-Real Gap and Domain Randomization"}),"\n",(0,t.jsxs)(n.p,{children:["Training complex robot behaviors, especially with RL, often requires millions of interactions. Running these interactions on physical hardware is time-consuming, expensive, and can cause wear and tear. Therefore, training is frequently done in simulation. However, a significant challenge arises when transferring these trained policies from simulation to the real world: the ",(0,t.jsx)(n.strong,{children:"Sim-to-Real Gap"}),"."]}),"\n",(0,t.jsx)(n.h4,{id:"what-is-the-sim-to-real-gap",children:"What is the Sim-to-Real Gap?"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"Sim-to-Real Gap"})," refers to the discrepancy between the simulated environment and the real world. Differences in physics parameters, sensor noise, lighting conditions, object properties, and even minor discrepancies in robot kinematics can cause a policy learned in simulation to perform poorly or fail entirely on a real robot."]}),"\n",(0,t.jsx)(n.h4,{id:"mitigating-the-sim-to-real-gap-with-domain-randomization",children:"Mitigating the Sim-to-Real Gap with Domain Randomization"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"})," is a key technique to bridge the Sim-to-Real Gap. Instead of trying to make the simulator perfectly match reality (which is often impossible or too complex), domain randomization makes the simulator ",(0,t.jsx)(n.em,{children:"so varied"})," that the real world appears to the robot as just another variation it has seen during training."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Technique"}),": During simulation training, various parameters of the environment and robot are randomized. This includes:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Texture, color, and size of objects."}),"\n",(0,t.jsx)(n.li,{children:"Lighting conditions."}),"\n",(0,t.jsx)(n.li,{children:"Sensor noise characteristics."}),"\n",(0,t.jsx)(n.li,{children:"Masses, frictions, and other physics properties."}),"\n",(0,t.jsx)(n.li,{children:"Minor perturbations in robot joint limits or sensor placements."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By training on a diverse set of randomized simulations, the RL agent learns a policy that is robust to variations and generalizes better to the novel (but statistically similar) real-world environment."}),"\n",(0,t.jsx)(n.h3,{id:"3-deployment-workflow-from-training-to-jetson-orin",children:"3. Deployment Workflow: From Training to Jetson Orin"}),"\n",(0,t.jsxs)(n.p,{children:["Once a robot controller is successfully trained in simulation, the next crucial step is to deploy it to physical hardware for real-world operation. NVIDIA's ",(0,t.jsx)(n.strong,{children:"Jetson Orin"})," is an ideal edge device for this purpose, offering high-performance AI inference capabilities in a compact, power-efficient form factor."]}),"\n",(0,t.jsx)(n.h4,{id:"workflow-stages",children:"Workflow Stages:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Training in Simulation (Cloud/Workstation)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The RL agent is trained in a high-fidelity simulator like Isaac Sim (running on a powerful workstation or cloud GPU instance). This stage leverages the computational power for rapid iteration and massive data generation via domain randomization."}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tools"}),": Isaac Sim, various RL frameworks (e.g., Isaac Gym, Stable Baselines3)."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Model Export/Serialization"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"After training, the learned policy (e.g., a neural network) needs to be exported from the training framework into a deployable format. Common formats include ONNX, TensorRT, or directly as a saved model file (e.g., TensorFlow SavedModel, PyTorch Script)."}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tools"}),": ONNX exporter, TensorRT builder, framework-specific model savers."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Downloading Weights"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The exported model weights (the learned parameters of the neural network) are then downloaded from the cloud training environment or transferred from the workstation to the target edge device (Jetson Orin)."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Flashing onto Local Jetson Orin Kit"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The model weights and the necessary inference runtime (e.g., TensorRT runtime, ONNX Runtime) are loaded onto the Jetson Orin."}),"\n",(0,t.jsx)(n.li,{children:"The robot's control software on the Jetson Orin then uses this loaded model for real-time inference, generating actions based on sensor inputs."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-the-critical-role-of-latency",children:"4. The Critical Role of Latency"}),"\n",(0,t.jsxs)(n.p,{children:["When controlling real robots from a trained AI model, ",(0,t.jsx)(n.strong,{children:"latency"})," is paramount."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),": Latency refers to the delay between a robot's perception (sensor input) and its subsequent action (motor command). High latency can lead to unstable control, collisions, or an inability to respond effectively to dynamic changes in the environment."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Impact"}),": When a trained model is deployed to an edge device like Jetson Orin, the inference happens locally, drastically reducing latency compared to sending data to the cloud and waiting for a response. This low latency is essential for achieving responsive and precise real-time robot control."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary-key-points",children:"Summary Key Points"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reinforcement Learning (RL)"})," is a primary method for training robot controllers in simulation."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.strong,{children:"Sim-to-Real Gap"})," is a key challenge when transferring policies from simulation to reality."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"})," is a technique to mitigate the Sim-to-Real Gap by increasing simulation diversity."]}),"\n",(0,t.jsxs)(n.li,{children:["A typical deployment workflow involves training in simulation (cloud/workstation), exporting models, downloading weights, and flashing them onto an edge device like ",(0,t.jsx)(n.strong,{children:"Jetson Orin"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"})," is a critical factor for real-time robot control, making edge deployment essential."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"readingresearch-references-apa-7th-edition",children:"Reading/Research References (APA 7th Edition)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,t.jsx)(n.em,{children:"Reinforcement Learning: An Introduction"})," (2nd ed.). MIT Press."]}),"\n",(0,t.jsxs)(n.li,{children:["Tobin, J., et al. (2017). ",(0,t.jsx)(n.em,{children:"Domain randomization for transferring deep neural networks from simulation to the real world"}),". IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)."]}),"\n",(0,t.jsxs)(n.li,{children:["NVIDIA. (n.d.). ",(0,t.jsx)(n.em,{children:"Jetson Orin Developer Kit"}),". Retrieved from ",(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/embedded/jetson-orin-developer-kit",children:"https://developer.nvidia.com/embedded/jetson-orin-developer-kit"})]}),"\n",(0,t.jsxs)(n.li,{children:["ROS 2 Documentation. (n.d.). ",(0,t.jsx)(n.em,{children:"Nav2"}),". Retrieved from ",(0,t.jsx)(n.a,{href:"https://navigation.ros.org/",children:"https://navigation.ros.org/"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var o=i(6540);const t={},r=o.createContext(t);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);